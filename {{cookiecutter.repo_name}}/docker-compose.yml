# Docker-compose configuration for development

version: '2.1'

services:
  django:
    build:
      context: .
      dockerfile: Dockerfile-django
    env_file: .env
    ports:
      - "${DJANGO_PORT-8000}:80"
    volumes:
      - "./{{cookiecutter.repo_name}}:/app"
      {%- if cookiecutter.frontend_style == "webapp" %}
      - "./webapp/static:/app/static"
      - "./webapp/webapp:/app/webapp"{% endif %}
      - ".data/media:/files/media"
    environment:
      - PYTHONUNBUFFERED=0
    # Add -Wall option to see the (deprecation) warnings
    # command: python -Wall manage.py runserver 0.0.0.0:80
    command: python manage.py runserver 0.0.0.0:80
    entrypoint: /usr/bin/wait-for-it.sh postgres:5432 -t 60 --
    depends_on:
      - postgres
      - redis
      - mailhog

  node:
    build:
      context: .
      dockerfile: Dockerfile-node
    env_file: .env
    {%- if cookiecutter.frontend_style == "webapp" and cookiecutter.webapp_include_storybook == 'yes' %}
    ports:
      - "6006:80"{% endif %}
    {%- if cookiecutter.frontend_style == "spa" %}
    ports:
      - "8000:8000"
      - "8001:8001"{% endif %}
    volumes:
      - "./{% if cookiecutter.frontend_style == "spa" %}app{% else %}webapp{% endif %}:/app"
      - ".data/node_modules:/app/node_modules"
      - ".data/yarn:/usr/local/share/.cache/yarn"

  postgres:
    image: postgres:{{cookiecutter.postgres_version}}
    # Comment in the following lines to connect to your Dockerized instance of Postgres from your host machine.
    # Change the host port (before colon) if you have a local instance of Postgres running on that port.
    # ports:
    #     - "5432:5432"
    volumes:
      - ".data/postgres:/var/lib/postgresql/data"
      - ".data/db-mirror:/db-mirror"  # Used by ansible mirror playbook
    environment:
      # credentials taken from .env file
      POSTGRES_USER: "${DJANGO_DATABASE_USER-'{{ cookiecutter.repo_name }}'}"
      POSTGRES_PASSWORD: "${DJANGO_DATABASE_PASSWORD-'{{ cookiecutter.repo_name }}'}"
  {%- if cookiecutter.include_celery == 'yes' %}

  celery:
    build:
      context: .
      dockerfile: Dockerfile-django
    env_file: .env
    volumes:
      - "./{{cookiecutter.repo_name}}:/app"
      - ".data/media:/files/media"
    depends_on:
      - postgres
      - redis
    environment:
      - PYTHONUNBUFFERED=0
    entrypoint: /usr/bin/wait-for-it.sh postgres:5432 -t 60 --
    command: celery --app {{cookiecutter.repo_name}} worker --autoscale 6,2 --loglevel INFO

  celery_beat:
    build:
      context: .
      dockerfile: Dockerfile-django
    env_file: .env
    container_name: {{cookiecutter.repo_name}}_celery_beat
    volumes:
      - "./{{cookiecutter.repo_name}}:/app"
      - ".data/celery:/celery"
    depends_on:
      - redis
    # Disable pidfile by specifying an empty one. We used fixed container_name which provides single-running-process
    #  guarantee and the lack of pidfile ensures that Celery Beat starts even if the Docker container was killed and
    #  then restarted (in which case the pidfile would still be present).
    command: celery --app {{cookiecutter.repo_name}} beat --loglevel INFO --pidfile= --schedule /celery/celerybeat-schedule
  {%- endif %}

  redis:
    image: redis:4.0.10-alpine
    sysctls:
      # To allow maintaining TCP backlog setting that defaults to 511
      net.core.somaxconn: 512
    volumes:
      - ".data/redis:/data"

  mailhog:
    image: mailhog/mailhog:v1.0.0
    ports:
      - "${MAILHOG_PORT-8025}:8025"
    logging:
      driver: "none"
