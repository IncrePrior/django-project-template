# Docker-compose configuration for production

version: '2'

services:
    django:
        build:
            context: .
            dockerfile: Dockerfile-django.production
        container_name: {{cookiecutter.repo_name}}_django
        restart: unless-stopped
        volumes:
            - "./{{cookiecutter.repo_name}}/app/build:/app/app/build"
            - "./{{cookiecutter.repo_name}}/app/webpack-stats.json:/app/app/webpack-stats.json"
            - "/var/lib/docker-nginx/files/{{cookiecutter.repo_name}}/media:/files/media"
            - "/var/lib/docker-nginx/files/{{cookiecutter.repo_name}}/assets:/files/assets"
            - "/var/log/{{cookiecutter.repo_name}}:/var/log/{{cookiecutter.repo_name}}"
        # For some reason the command also has to be specified here, otherwise the entrypoint+command combination won't
        #  work.
        entrypoint: /usr/bin/wait-for-it.sh postgres:5432 -t 60 --
        command: gunicorn {{cookiecutter.repo_name}}.wsgi:application --workers 2 --bind :80
        networks:
            - default
            - {{cookiecutter.repo_name}}_nginx
            - {{cookiecutter.repo_name}}_postgres
        depends_on:
            - redis
        external_links:
            - postgres-10:postgres

    node:
        build:
            context: .
            dockerfile: Dockerfile-node.production
        # Make this service no-op as we don't actually want it running but do want to use docker-compose file to define
        #  volumes etc.
        restart: "no"
        command: "true"
        volumes:
            - "./{{cookiecutter.repo_name}}/app:/app/app"
            - "./{{cookiecutter.repo_name}}/static:/app/static:ro"
        # Node container won't be part of any networks
        networks: []
    {%- if cookiecutter.include_celery == 'yes' %}

    celery:
        build:
            context: .
            dockerfile: Dockerfile-django.production
        restart: unless-stopped
        volumes:
            - "/var/log/{{cookiecutter.repo_name}}:/var/log/{{cookiecutter.repo_name}}"
        networks:
            - default
            - {{cookiecutter.repo_name}}_postgres
        depends_on:
            - redis
        external_links:
            - postgres-10:postgres
        entrypoint: /usr/bin/wait-for-it.sh postgres:5432 -t 60 --
        command: celery worker --app {{cookiecutter.repo_name}} --autoscale 6,2 --loglevel INFO

    celery_beat:
        build:
            context: .
            dockerfile: Dockerfile-django.production
        container_name: {{cookiecutter.repo_name}}_celery_beat
        restart: unless-stopped
        volumes:
            - "/var/lib/docker-{{cookiecutter.repo_name}}/celery:/celery"
            - "/var/log/{{cookiecutter.repo_name}}:/var/log/{{cookiecutter.repo_name}}"
        networks:
            - default
        depends_on:
            - redis
        # Disable pidfile by specifying an empty one. We used fixed container_name which provides single-running-process
        #  guarantee and the lack of pidfile ensures that Celery Beat starts even if the Docker container was killed and
        #  then restarted (in which case the pidfile would still be present).
        command: celery beat --app {{cookiecutter.repo_name}} --loglevel INFO --logfile /var/log/{{cookiecutter.repo_name}}/celery-beat.log --pidfile= --schedule /celery/celerybeat-schedule
    {%- endif %}

    redis:
        image: redis:3.2.8-alpine
        restart: unless-stopped
        volumes:
            - "/var/lib/docker-{{cookiecutter.repo_name}}/redis:/data"
        networks:
            - default

# NB: These networks must be created by fabfile and contain the global nginx/postgres containers.
# Keep it in sync with fabfile!
networks:
    default:
        external:
            name: {{cookiecutter.repo_name}}_default
    {{cookiecutter.repo_name}}_nginx:
        external: true
    {{cookiecutter.repo_name}}_postgres:
        external: true
